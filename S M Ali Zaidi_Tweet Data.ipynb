{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S M Ali Zaidi\n",
    "# Tweets Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "el1V1B6DRaOE",
    "outputId": "dc7dc5ce-95d8-41b8-be3e-e97bf7514479"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sohaib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sohaib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Sohaib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tZLTcZvZDPIF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8sgVpU9pDSr7",
    "outputId": "2fe57757-cadb-4553-fa8c-8afba68bb3c8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>message to examine</th>\n",
       "      <th>label (depression result)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>288</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                 message to examine  \\\n",
       "0    106  just had a real good moment. i missssssssss hi...   \n",
       "1    217         is reading manga  http://plurk.com/p/mzp1e   \n",
       "2    220  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
       "3    288  @lapcat Need to send 'em to my accountant tomo...   \n",
       "4    540      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
       "\n",
       "   label (depression result)  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/Sohaib/Documents/ATOM CAMP/Natural Language Processing/Assignment 1/Tweets/sentiment_tweets3.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sohaib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  message to examine  \\\n",
      "0  just had a real good moment. i missssssssss hi...   \n",
      "1         is reading manga  http://plurk.com/p/mzp1e   \n",
      "2  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
      "3  @lapcat Need to send 'em to my accountant tomo...   \n",
      "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
      "\n",
      "                                      text_processed  \n",
      "0                       [real good moment miss much]  \n",
      "1                                    [reading manga]  \n",
      "2                                     [comeagainjen]  \n",
      "3  [lapcat need send em accountant tomorrow oddly...  \n",
      "4                [add myspace myspacecomlookthunder]  \n"
     ]
    }
   ],
   "source": [
    "# Initializing stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove special characters and punctuations\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'  # Keep only alphanumeric characters and spaces\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Function to remove URLs\n",
    "def remove_urls(text):\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Function for lowercasing\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function for tokenization\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Function for stemming\n",
    "def stem_text(tokens):\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Function for lemmatization\n",
    "def lemmatize_text(tokens):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Function for sentence segmentation\n",
    "def segment_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# Combined preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = lowercase_text(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_stop_words(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    stemmed_text = stem_text(tokens)  #stemming\n",
    "    lemmatized_text = lemmatize_text(tokens)  # lemmatization\n",
    "    sentences = segment_sentences(lemmatized_text)  # Sentence segmentation on lemmatized text\n",
    "    return sentences  # Returns a list of sentences\n",
    "\n",
    "# Applying preprocessing to the DataFrame\n",
    "data['text_processed'] = data['message to examine'].apply(preprocess_text)\n",
    "\n",
    "# Displaying the first few rows to verify\n",
    "print(data[['message to examine', 'text_processed']].head())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
